{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruments and Intermediate Measurements Tutorial\n",
    "This tutorial will demonstrate how perform tomography on models which, in addition to normal gates, contain *quantum instruments*.  Quantum instruments are maps that act on a qubit state (density matrix) and produce a qubit state along with a classical outcome.  That is, instruments are maps from $\\mathcal{B}(\\mathcal{H})$, the space of density matrices, to $\\mathcal{B}(\\mathcal{H}) \\otimes K(n)$, where $K(n)$ is a classical space of $n$ elements.\n",
    "\n",
    "In pyGSTi, instruments are represented as collections of gates, one for each classical \"outcome\" of the instrument.  This tutorial will demonstrate how to add instruments to `Model` objects, compute probabilities using such `Model`s, and ultimately perform tomography on them.  We'll start with a few familiar imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsti\n",
    "from pygsti.modelpacks import smq1Q_XYI as std\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument construction\n",
    "Next, we'll add an instrument to our \"standard\" model - a 1-qubit model containing $I$, $X(\\pi/2)$, and $Y(\\pi/2)$ gates.  The ideal instrument will be named `\"Iz\"` (all instrument names must begin with `\"I\"`), and consist of perfect projectors onto the 0 and 1 states.  Instead of labelling the associated outcomes \"0\" and \"1\", which might me most logical, we'll name them \"p0\" and \"p1\" so it's easier to distinguish them from the final POVM outcomes which *are* labelled \"0\" and \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy so we don't modify the original\n",
    "target_model = std.target_model()\n",
    "\n",
    "#Create and add the ideal instrument\n",
    "E0 = target_model.effects['0']\n",
    "E1 = target_model.effects['1']\n",
    " # Alternate indexing that uses POVM label explicitly\n",
    " # E0 = target_model['Mdefault']['0']  # 'Mdefault' = POVM label, '0' = effect label\n",
    " # E1 = target_model['Mdefault']['1']\n",
    "Gmz_plus = np.dot(E0,E0.T) #note effect vectors are stored as column vectors\n",
    "Gmz_minus = np.dot(E1,E1.T)\n",
    "target_model[('Iz',0)] = pygsti.obj.Instrument({'p0': Gmz_plus, 'p1': Gmz_minus})\n",
    "\n",
    "#For later use, record the identity POVM vector\n",
    "povm_ident = target_model.effects['0'] + target_model.effects['1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate some simulated data later on, we'll now create a noisy version of `target_model` by depolarizing the state preparation, gates, and POVM, and also rotating the basis that is measured by the instrument and POVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_noisy = target_model.depolarize(op_noise=0.01, spam_noise=0.01)\n",
    "mdl_noisy.effects.depolarize(0.01)  #because above call only depolarizes the state prep, not the POVMs\n",
    "\n",
    "# add a rotation error to the POVM\n",
    "Uerr = pygsti.rotation_gate_mx([0,0.02,0])\n",
    "mdl_noisy.effects['0'] = np.dot(mdl_noisy.effects['0'].T,Uerr).T\n",
    "mdl_noisy.effects['1'] = povm_ident - mdl_noisy.effects['0']\n",
    "\n",
    "#Could also do this:\n",
    "#E0 = np.dot(mdl_noisy['Mdefault']['0'].T,Uerr).T\n",
    "#E1 = povm_ident - E0\n",
    "#mdl_noisy['Mdefault'] = pygsti.obj.UnconstrainedPOVM({'0': E0, '1': E1})\n",
    "\n",
    "# Use the same rotated effect vectors to \"rotate\" the instrument Iz too\n",
    "E0 = mdl_noisy.effects['0']\n",
    "E1 = mdl_noisy.effects['1']\n",
    "Gmz_plus = np.dot(E0,E0.T)\n",
    "Gmz_minus = np.dot(E1,E1.T)\n",
    "mdl_noisy[('Iz',0)] = pygsti.obj.Instrument({'p0': Gmz_plus, 'p1': Gmz_minus})\n",
    "\n",
    "#print(mdl_noisy) #print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating probabilities \n",
    "Instrument labels (e.g. `\"Iz\"`) may be included within `Circuit` objects, and `Model` objects are able to compute probabilities for them just like normal (non-instrument) operation sequences.  The difference is that probabilities are labeled by tuples of instrument and POVM outcomes - referred to as **\"outcome tuples\"** - one for each instrument and one for the final POVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(target_model.probs( pygsti.obj.Circuit(( ('Gxpi2',0) , ('Iz',0) )) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(target_model.probs( pygsti.obj.Circuit(( ('Iz',0), ('Gxpi2',0) , ('Iz',0) )) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, pyGSTi *always* labels probabilties using outcome tuples, it's just that in the non-instrument case they're always 1-tuples and by `OutcomeLabelDict` magic can be treated as if they were just strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = target_model.probs( pygsti.obj.Circuit([('Gxpi2',0)]) )\n",
    "print(\"probs = \",dict(probs))\n",
    "print(\"probs['0'] = \", probs['0']) #This works...\n",
    "print(\"probs[('0',)] = \", probs[('0',)]) # and so does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing tomography\n",
    "\n",
    "### Simulated data generation\n",
    "Now let's perform tomography on a model that includes instruments.  First, we'll generate some data using `mdl_noisy` in exactly the same way as we would for any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "germs = std.germs()\n",
    "prep_fiducials = std.prep_fiducials()\n",
    "meas_fiducials = std.meas_fiducials()\n",
    "max_lengths = [1] # keep it simple & fast\n",
    "\n",
    "lsgst_list = pygsti.construction.make_lsgst_experiment_list(\n",
    "    mdl_noisy,prep_fiducials,meas_fiducials,germs,max_lengths)\n",
    "\n",
    "#print(\"LinearOperator sequences:\")\n",
    "#print(lsgst_list) #note that this contains LGST strings with \"Iz\"\n",
    "\n",
    "#Create the DataSet\n",
    "ds = pygsti.construction.generate_fake_data(mdl_noisy,lsgst_list,1000,'multinomial',seed=2018)\n",
    "\n",
    "#Write it to a text file to demonstrate the format:\n",
    "pygsti.io.write_dataset(\"../../tutorial_files/intermediate_meas_dataset.txt\",ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the format of [intermediate_meas_dataset.txt](../../tutorial_files/intermediate_meas_dataset.txt), which includes a column for each distinct outcome tuple.  Since not all experiments contain data for all outcome tuples, the `\"--\"` is used as a placeholder.  Now that the data is generated, we run LGST or LSGST just like we would for any other model:\n",
    "\n",
    "### LGST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run LGST\n",
    "mdl_lgst = pygsti.do_lgst(ds, prep_fiducials, meas_fiducials, target_model)\n",
    "#print(mdl_lgst)\n",
    "\n",
    "#Gauge optimize the result to the true data-generating model (mdl_noisy),\n",
    "# and compare.  Mismatch is due to finite sample noise.\n",
    "mdl_lgst_opt = pygsti.gaugeopt_to_target(mdl_lgst,mdl_noisy)\n",
    "print(mdl_noisy.strdiff(mdl_lgst_opt))\n",
    "print(\"Frobdiff after GOpt = \",mdl_noisy.frobeniusdist(mdl_lgst_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-sequence GST\n",
    "Instruments just add parameters to a `Model` like gates, state preparations, and POVMs do.  The total number of parameters in our model is \n",
    "\n",
    "$4$ (prep) + $2\\times 4$ (2 effects) + $5\\times 16$ (3 gates and 2 instrument members) $ = 92$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run long sequence GST\n",
    "results = pygsti.do_long_sequence_gst(ds,target_model,prep_fiducials,meas_fiducials,germs,max_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare estimated model (after gauge opt) to data-generating one\n",
    "mdl_est = results.estimates['default'].models['go0']\n",
    "mdl_est_opt = pygsti.gaugeopt_to_target(mdl_est,mdl_noisy)\n",
    "print(\"Frobdiff after GOpt = \", mdl_noisy.frobeniusdist(mdl_est_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same analysis can be done for a trace-preserving model, whose instruments are constrained to *add* to a perfectly trace-preserving map.  The number of parameters in the model are now:  \n",
    "\n",
    "$3$ (prep) + $1\\times 4$ (effect and complement) + $3\\times 12$ (3 gates) + $(2\\times 16 - 3)$ (TP instrument) $ = 71$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_targetTP = target_model.copy()\n",
    "mdl_targetTP.set_all_parameterizations(\"TP\")\n",
    "print(\"POVM type = \",type(mdl_targetTP[\"Mdefault\"]),\" Np=\",mdl_targetTP[\"Mdefault\"].num_params())\n",
    "print(\"Instrument type = \",type(mdl_targetTP[(\"Iz\",0)]),\" Np=\",mdl_targetTP[(\"Iz\",0)].num_params())\n",
    "print(\"Number of model parameters = \", mdl_targetTP.num_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsTP = pygsti.do_long_sequence_gst(ds,mdl_targetTP,prep_fiducials,meas_fiducials,germs,max_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again compare estimated model (after gauge opt) to data-generating one\n",
    "mdl_est = resultsTP.estimates['default'].models['go0']\n",
    "mdl_est_opt = pygsti.gaugeopt_to_target(mdl_est,mdl_noisy)\n",
    "print(\"Frobdiff after GOpt = \", mdl_noisy.frobeniusdist(mdl_est_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thats it!**  You've done tomography on a model with intermediate measurments (instruments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
