# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['pyjapt']

package_data = \
{'': ['*']}

setup_kwargs = {
    'name': 'pyjapt',
    'version': '0.1.2',
    'description': 'Just Another Parsing Tool Writ ten in Python',
    'long_description': '# Lexer and LR parser generator  "PyJapt"\n\nPyJapt is a lexer and parser generator developed to provide a solution not only to the creation of these pieces of the compilation process, but also to allow a custom syntactic and lexicographic error handling interface. For its construction we have been inspired by other parser generators such as yacc, bison, ply and antlr, for example.\n\nPyJapt revolves around the concept of grammar.\n\nTo define the nonterminals of the grammar we use the `add_non_terminal ()` method of the `Grammar` class.\n\n```python\nfrom pyjapt.grammar import Grammar\n\nG = Grammar()\n\nexpr = G.add_non_terminal(\'expr\', start_symbol=True)\nterm = G.add_non_terminal(\'term\')\nfact = G.add_non_terminal(\'fact\')\n```\n\nTo define the terminals of our grammar we will use the `add_terminal ()` method of the `Grammar` class. This method receives the name of the non-terminal as the first parameter and a regular expression for the lexicographic analyzer as an optional parameter. In case the second parameter is not provided, the regular expression will be the literal name of the terminal.\n\n```python\nplus = G.add_terminals(\'+\')\nminus = G.add_terminals(\'-\')\nstar = G.add_terminals(\'*\')\ndiv = G.add_terminals(\'/\')\n\nnum = G.add_terminal(\'int\', regex=r\'\\d+\')\n```\n\nIf we have a set of terminals whose regular expression matches their own name we can encapsulate them with the `add_terminals()` function of the `Grammar` class.\n\n```python\nplus, minus, star, div = G.add_terminals(\'+ - * /\')\nnum = G.add_terminal(\'int\', regex=r\'\\d+\')\n```\n\nIt may also be the case that we want to apply a rule when a specific terminal is found, for this PyJapt gives us the `terminal()` function decorator of the `Grammar` class that receives the terminal name and regular expression. The decorated function must receive as a parameter a reference to the lexer to be able to modify parameters such as the row and column of the terminals or the parser reading position and return a `Token`, if this token is not returned it will be ignored.\n\n```python\n@G.terminal(\'int\', r\'\\d+\')\ndef id_terminal(lexer):\n    lexer.column += len(lexer.token.lex)\n    lexer.position += len(lexer.token.lex)\n    lexer.token.lex = int(lexer.token.lex)\n    return lexer.token\n```\n\nWe can also use this form of terminal definition to skip certain characters or tokens, we just need to ignore the return in the method.\n\n```python\n##################\n# Ignored Tokens #\n##################\n@G.terminal(\'newline\', r\'\\n+\')\ndef newline(lexer):\n    lexer.lineno += len(lexer.token.lex)\n    lexer.position += len(lexer.token.lex)\n    lexer.column = 1\n\n\n@G.terminal(\'whitespace\', r\' +\')\ndef whitespace(lexer):\n    lexer.column += len(lexer.token.lex)\n    lexer.position += len(lexer.token.lex)\n\n\n@G.terminal(\'tabulation\', r\'\\t+\')\ndef tab(lexer):\n    lexer.column += 4 * len(lexer.token.lex)\n    lexer.position += len(lexer.token.lex)\n```\n\nTo define the productions of our grammar we can use an attributed or not attributed form:\n\n```python\n# This is an unattributed grammar using previously declared variables\nexpr %= expr + plus + term\nexpr %= expr + minus + term\nexpr %= term\n\nterm %= term + star + fact\nterm %= term + div + fact\nterm %= fact\n\nfact %= num\n\n# A little easier ...\n# Each symbol in the production string must be separated by a blank space\nexpr %= \'expr + term\'\nexpr %= \'expr - term\'\nexpr %= \'term\'\n\nterm %= \'term * factor\'\nterm %= \'term / factor\'\nterm %= \'fact\'\n\nfact %= \'int\'\n\n# This is an attributed grammar\nexpr %= \'expr + term\', lambda s: s[1] + s[3]\nexpr %= \'expr - term\', lambda s: s[1] + s[3]\nexpr %= \'term\', lambda s: s[1]\n\nterm %= \'term * factor\', lambda s: s[1] + s[3]\nterm %= \'term / factor\', lambda s: s[1] + s[3]\nterm %= \'fact\', lambda s: s[1]\n\nfact %= \'int\', lambda s: int(s[1])\n\n# We can also attribute a function to define a semantic rule\n# This function should receive as parameter `s` which is a reference to a\n# list with the semantic rules of each symbol of the production.\n# To separate the symbol from the head of the body of the expression\n# use the second symbol `->`\n@G.production(\'expr -> expr + term\')\ndef expr_prod(s):\n    print(\'Adding an expression and a term ;)\')\n    return s[1] + s[3]\n```\n\n## Handling of lexicographic and syntactic errors\n\nAn important part of the parsing process is handling errors. For this we can do the parser by hand and insert the error report, since techniques such as `Panic Recovery Mode` which implements `PyJapt` only allow the execution of our parser not to stop, to give specific error reports `PyJapt` offers the creation of erroneous productions to report common errors in a programming language such as the lack of a `;`, an unknown operator, etc. For this our grammar must activate the terminal error flag.\n\n```python\nG.add_terminal_error() # Add the error terminal to the grammar.\n\n# Example of a possible error production\n@G.production("instruction -> let id = expr error")\ndef attribute_error(s):\n    # With this line we report the error message\n    # As the semantic rule of s [5] is the token itself (because it is a terminal), so we have access\n    # to your lexeme, token type, line and column.\n    s.set_error(5, f"{s[5].line, s[5].column} - SyntacticError: Expected \';\' instead of \'{s[5].lex}\'")\n\n    # With this line we allow to continue creating a node of the ast to\n    # be able to detect semantic errors despite syntactic errors\n    return LetInstruction(s[2], s[4])\n```\n\nTo report lexicographical errors the procedure is quite similar we only define a token that contains an error, in this example a multi-line comment that contains an end of string.\n\n```python\n@G.terminal(\'comment_error\', r\'\\(\\*(.|\\n)*$\')\ndef comment_eof_error(lexer):\n    lexer.contain_errors = True\n    lex = lexer.token.lex\n    for s in lex:\n        if s == \'\\n\':\n            lexer.lineno += 1\n            lexer.column = 0\n        lexer.column = 1\n    lexer.position += len(lex)\n    lexer.print_error(f\'{lexer.lineno, lexer.column} -LexicographicError: EOF in comment\')\n```\n\nAnd to report general errors during the tokenization process we can use the `lexical_error` decorator.\n\n```python\n@G.lexical_error\ndef lexical_error(lexer):\n    lexer.print_error(f\'{lexer.lineno, lexer.column} -LexicographicError: ERROR "{lexer.token.lex}"\')\n    lexer.column += 1\n    lexer.position += 1\n```\n',
    'author': 'Alejandro Klever',
    'author_email': 'alejandroklever4197@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/alejandroklever/PyJapt',
    'packages': packages,
    'package_data': package_data,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
